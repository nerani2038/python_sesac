{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024.07.05\n",
      "2024.07.05\n",
      "5\n",
      "Fetching page 1 for date 2024.07.05...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.05...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.05...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.05...\n",
      "31 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "21개의 뉴스 제목이 './data/news_titles_2024.07.05.json' 파일에 저장되었습니다.\n",
      "6\n",
      "Fetching page 1 for date 2024.07.06...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.06...\n",
      "11 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "7개의 뉴스 제목이 './data/news_titles_2024.07.06.json' 파일에 저장되었습니다.\n",
      "7\n",
      "Fetching page 1 for date 2024.07.07...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.07...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.07...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.07...\n",
      "31 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "28개의 뉴스 제목이 './data/news_titles_2024.07.07.json' 파일에 저장되었습니다.\n",
      "8\n",
      "Fetching page 1 for date 2024.07.08...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.08...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.08...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.08...\n",
      "31 200\n",
      "Fetching page 5 for date 2024.07.08...\n",
      "41 200\n",
      "Fetching page 6 for date 2024.07.08...\n",
      "51 403\n",
      "HTTP 요청 실패: 403\n",
      "50개의 뉴스 제목이 './data/news_titles_2024.07.08.json' 파일에 저장되었습니다.\n",
      "9\n",
      "Fetching page 1 for date 2024.07.09...\n",
      "1 403\n",
      "HTTP 요청 실패: 403\n",
      "0개의 뉴스 제목이 './data/news_titles_2024.07.09.json' 파일에 저장되었습니다.\n",
      "10\n",
      "Fetching page 1 for date 2024.07.10...\n",
      "1 403\n",
      "HTTP 요청 실패: 403\n",
      "0개의 뉴스 제목이 './data/news_titles_2024.07.10.json' 파일에 저장되었습니다.\n",
      "11\n",
      "Fetching page 1 for date 2024.07.11...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.11...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.11...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.11...\n",
      "31 200\n",
      "Fetching page 5 for date 2024.07.11...\n",
      "41 200\n",
      "Fetching page 6 for date 2024.07.11...\n",
      "51 200\n",
      "Fetching page 7 for date 2024.07.11...\n",
      "61 200\n",
      "Fetching page 8 for date 2024.07.11...\n",
      "71 200\n",
      "Fetching page 9 for date 2024.07.11...\n",
      "81 200\n",
      "Fetching page 10 for date 2024.07.11...\n",
      "91 403\n",
      "HTTP 요청 실패: 403\n",
      "90개의 뉴스 제목이 './data/news_titles_2024.07.11.json' 파일에 저장되었습니다.\n",
      "12\n",
      "Fetching page 1 for date 2024.07.12...\n",
      "1 403\n",
      "HTTP 요청 실패: 403\n",
      "0개의 뉴스 제목이 './data/news_titles_2024.07.12.json' 파일에 저장되었습니다.\n",
      "13\n",
      "Fetching page 1 for date 2024.07.13...\n",
      "1 403\n",
      "HTTP 요청 실패: 403\n",
      "0개의 뉴스 제목이 './data/news_titles_2024.07.13.json' 파일에 저장되었습니다.\n",
      "14\n",
      "Fetching page 1 for date 2024.07.14...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.14...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.14...\n",
      "21 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "14개의 뉴스 제목이 './data/news_titles_2024.07.14.json' 파일에 저장되었습니다.\n",
      "15\n",
      "Fetching page 1 for date 2024.07.15...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.15...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.15...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.15...\n",
      "31 200\n",
      "Fetching page 5 for date 2024.07.15...\n",
      "41 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "36개의 뉴스 제목이 './data/news_titles_2024.07.15.json' 파일에 저장되었습니다.\n",
      "16\n",
      "Fetching page 1 for date 2024.07.16...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.16...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.16...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.16...\n",
      "31 200\n",
      "Fetching page 5 for date 2024.07.16...\n",
      "41 403\n",
      "HTTP 요청 실패: 403\n",
      "40개의 뉴스 제목이 './data/news_titles_2024.07.16.json' 파일에 저장되었습니다.\n",
      "17\n",
      "Fetching page 1 for date 2024.07.17...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.17...\n",
      "11 403\n",
      "HTTP 요청 실패: 403\n",
      "10개의 뉴스 제목이 './data/news_titles_2024.07.17.json' 파일에 저장되었습니다.\n",
      "18\n",
      "Fetching page 1 for date 2024.07.18...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.18...\n",
      "11 403\n",
      "HTTP 요청 실패: 403\n",
      "10개의 뉴스 제목이 './data/news_titles_2024.07.18.json' 파일에 저장되었습니다.\n",
      "19\n",
      "Fetching page 1 for date 2024.07.19...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.19...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.19...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.19...\n",
      "31 200\n",
      "Fetching page 5 for date 2024.07.19...\n",
      "41 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "40개의 뉴스 제목이 './data/news_titles_2024.07.19.json' 파일에 저장되었습니다.\n",
      "20\n",
      "Fetching page 1 for date 2024.07.20...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.20...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.20...\n",
      "21 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "13개의 뉴스 제목이 './data/news_titles_2024.07.20.json' 파일에 저장되었습니다.\n",
      "21\n",
      "Fetching page 1 for date 2024.07.21...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.21...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.21...\n",
      "21 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "13개의 뉴스 제목이 './data/news_titles_2024.07.21.json' 파일에 저장되었습니다.\n",
      "22\n",
      "Fetching page 1 for date 2024.07.22...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.22...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.22...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.22...\n",
      "31 200\n",
      "Fetching page 5 for date 2024.07.22...\n",
      "41 200\n",
      "Fetching page 6 for date 2024.07.22...\n",
      "51 200\n",
      "Fetching page 7 for date 2024.07.22...\n",
      "61 403\n",
      "HTTP 요청 실패: 403\n",
      "60개의 뉴스 제목이 './data/news_titles_2024.07.22.json' 파일에 저장되었습니다.\n",
      "23\n",
      "Fetching page 1 for date 2024.07.23...\n",
      "1 403\n",
      "HTTP 요청 실패: 403\n",
      "0개의 뉴스 제목이 './data/news_titles_2024.07.23.json' 파일에 저장되었습니다.\n",
      "24\n",
      "Fetching page 1 for date 2024.07.24...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.24...\n",
      "11 403\n",
      "HTTP 요청 실패: 403\n",
      "10개의 뉴스 제목이 './data/news_titles_2024.07.24.json' 파일에 저장되었습니다.\n",
      "25\n",
      "Fetching page 1 for date 2024.07.25...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.25...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.25...\n",
      "21 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "14개의 뉴스 제목이 './data/news_titles_2024.07.25.json' 파일에 저장되었습니다.\n",
      "26\n",
      "Fetching page 1 for date 2024.07.26...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.26...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.26...\n",
      "21 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "14개의 뉴스 제목이 './data/news_titles_2024.07.26.json' 파일에 저장되었습니다.\n",
      "27\n",
      "Fetching page 1 for date 2024.07.27...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.27...\n",
      "11 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "2개의 뉴스 제목이 './data/news_titles_2024.07.27.json' 파일에 저장되었습니다.\n",
      "28\n",
      "Fetching page 1 for date 2024.07.28...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.28...\n",
      "11 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "2개의 뉴스 제목이 './data/news_titles_2024.07.28.json' 파일에 저장되었습니다.\n",
      "29\n",
      "Fetching page 1 for date 2024.07.29...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.29...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.29...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.29...\n",
      "31 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "22개의 뉴스 제목이 './data/news_titles_2024.07.29.json' 파일에 저장되었습니다.\n",
      "30\n",
      "Fetching page 1 for date 2024.07.30...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.30...\n",
      "11 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "10개의 뉴스 제목이 './data/news_titles_2024.07.30.json' 파일에 저장되었습니다.\n",
      "31\n",
      "Fetching page 1 for date 2024.07.31...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.31...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.31...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.31...\n",
      "31 200\n",
      "더 이상 뉴스가 없습니다.\n",
      "25개의 뉴스 제목이 './data/news_titles_2024.07.31.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def get_news_titles(query, date):\n",
    "    base_url = \"https://search.naver.com/search.naver\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
    "    }\n",
    "    params = {\n",
    "        \"where\": \"news\",\n",
    "        \"query\": query,\n",
    "        \"sm\": \"tab_opt\",\n",
    "        \"sort\": \"0\",  # 관련도순 정렬\n",
    "        \"pd\": \"3\",  # 기간 설정\n",
    "        \"ds\": date,  # 시작 날짜\n",
    "        \"de\": date,  # 종료 날짜\n",
    "        \"start\": 1,  # 첫 페이지\n",
    "    }\n",
    "    titles = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Fetching page {page} for date {date}...\")\n",
    "            params[\"start\"] = (page - 1) * 10 + 1  # 페이지 계산\n",
    "            response = requests.get(base_url, headers=headers, params=params)\n",
    "            print(params[\"start\"], response.status_code)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"HTTP 요청 실패: {response.status_code}\")\n",
    "                break\n",
    "            # HTML 파싱\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # 뉴스 제목 추출\n",
    "            news_items = soup.select(\".news_tit\")  # 제목 링크에 해당하는 클래스\n",
    "            if not news_items:\n",
    "                print(\"더 이상 뉴스가 없습니다.\")\n",
    "                break\n",
    "            for item in news_items:\n",
    "                title = item.get(\"title\")\n",
    "                if title:\n",
    "                    titles.append(title)\n",
    "            # 다음 페이지로 이동\n",
    "            page += 1\n",
    "            # 네이버 뉴스 검색은 최대 400개 기사만 제공\n",
    "            if page > 40:\n",
    "                print(\"최대 페이지 제한 도달.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"페이지 {page} 크롤링 중 에러 발생: {e}\")\n",
    "            break\n",
    "    return titles\n",
    "def save_daily_news(query, start_date, end_date):\n",
    "    print(start_date)\n",
    "    print(end_date)\n",
    "    for date in range(5, 32):\n",
    "        print(date)\n",
    "        start_date = f\"2024.07.{date}\"\n",
    "        end_date = f\"2024.07.{date}\"\n",
    "\n",
    "        current_date = datetime.strptime(start_date, \"%Y.%m.%d\")\n",
    "        end_date = datetime.strptime(end_date, \"%Y.%m.%d\")\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime(\"%Y.%m.%d\")\n",
    "            titles = get_news_titles(query, date_str)\n",
    "            # 날짜별 JSON 파일 저장\n",
    "            output_file = f\"./data/news_titles_{date_str}.json\"\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(titles, file, ensure_ascii=False, indent=4)\n",
    "            print(f\"{len(titles)}개의 뉴스 제목이 '{output_file}' 파일에 저장되었습니다.\")\n",
    "            # 다음 날짜로 이동\n",
    "            current_date += timedelta(days=1)\n",
    "        time.sleep(2)\n",
    "def merge_json_files(start_date, end_date, output_file):\n",
    "    current_date = datetime.strptime(start_date, \"%Y.%m.%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y.%m.%d\")\n",
    "    all_titles = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y.%m.%d\")\n",
    "        input_file = f\"news_titles_{date_str}.json\"\n",
    "        try:\n",
    "            with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                titles = json.load(file)\n",
    "                all_titles.extend(titles)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"파일 '{input_file}'을(를) 찾을 수 없습니다.\")\n",
    "        current_date += timedelta(days=1)\n",
    "    # 모든 제목을 하나의 JSON 파일로 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(all_titles, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"모든 뉴스 제목이 '{output_file}' 파일에 저장되었습니다.\")\n",
    "# 실행\n",
    "query = \"아시아나항공\"\n",
    "start_date = \"2024.07.05\"\n",
    "end_date = \"2024.07.05\"\n",
    "\n",
    "# 날짜별 JSON 파일 저장\n",
    "save_daily_news(query, start_date, end_date)\n",
    "# 모든 JSON 파일 병합\n",
    "# merge_json_files(start_date, end_date, \"all_asiana_news_titles.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for date 2024.07.02...\n",
      "1 200\n",
      "Fetching page 2 for date 2024.07.02...\n",
      "11 200\n",
      "Fetching page 3 for date 2024.07.02...\n",
      "21 200\n",
      "Fetching page 4 for date 2024.07.02...\n",
      "31 200\n",
      "Fetching page 5 for date 2024.07.02...\n",
      "41 200\n",
      "Fetching page 6 for date 2024.07.02...\n",
      "51 200\n",
      "Fetching page 7 for date 2024.07.02...\n",
      "61 200\n",
      "Fetching page 8 for date 2024.07.02...\n",
      "71 403\n",
      "HTTP 요청 실패: 403\n",
      "70개의 뉴스 제목이 './data/news_titles_2024.07.02.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_news_titles(query, date):\n",
    "    base_url = \"https://search.naver.com/search.naver\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
    "    }\n",
    "    params = {\n",
    "        \"where\": \"news\",\n",
    "        \"query\": query,\n",
    "        \"sm\": \"tab_opt\",\n",
    "        \"sort\": \"0\",  # 관련도순 정렬\n",
    "        \"pd\": \"3\",  # 기간 설정\n",
    "        \"ds\": date,  # 시작 날짜\n",
    "        \"de\": date,  # 종료 날짜\n",
    "        \"start\": 1,  # 첫 페이지\n",
    "    }\n",
    "    titles = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Fetching page {page} for date {date}...\")\n",
    "            params[\"start\"] = (page - 1) * 10 + 1  # 페이지 계산\n",
    "\n",
    "            if params[\"start\"] > 90:\n",
    "                print(\"최대 페이지 제한 도달.\")\n",
    "                break\n",
    "\n",
    "            response = requests.get(base_url, headers=headers, params=params)\n",
    "            print(params[\"start\"], response.status_code)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"HTTP 요청 실패: {response.status_code}\")\n",
    "                break\n",
    "            # HTML 파싱\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # 뉴스 제목 추출\n",
    "            news_items = soup.select(\".news_tit\")  # 제목 링크에 해당하는 클래스\n",
    "            if not news_items:\n",
    "                print(\"더 이상 뉴스가 없습니다.\")\n",
    "                break\n",
    "            for item in news_items:\n",
    "                title = item.get(\"title\")\n",
    "                if title:\n",
    "                    titles.append(title)\n",
    "            # 다음 페이지로 이동\n",
    "            page += 1\n",
    "            # 네이버 뉴스 검색은 최대 900개 기사만 제공\n",
    "            if page > 40:\n",
    "                print(\"최대 페이지 제한 도달.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"페이지 {page} 크롤링 중 에러 발생: {e}\")\n",
    "            break\n",
    "    return titles\n",
    "\n",
    "def save_daily_news(query, start_date, end_date):\n",
    "    current_date = datetime.strptime(start_date, \"%Y.%m.%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y.%m.%d\")\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y.%m.%d\")\n",
    "        titles = get_news_titles(query, date_str)\n",
    "        # 날짜별 JSON 파일 저장\n",
    "        output_file = f\"./data/news_titles_{date_str}.json\"\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(titles, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"{len(titles)}개의 뉴스 제목이 '{output_file}' 파일에 저장되었습니다.\")\n",
    "        # 다음 날짜로 이동\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "def merge_json_files(start_date, end_date, output_file):\n",
    "    current_date = datetime.strptime(start_date, \"%Y.%m.%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y.%m.%d\")\n",
    "    all_titles = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y.%m.%d\")\n",
    "        input_file = f\"news_titles_{date_str}.json\"\n",
    "        try:\n",
    "            with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                titles = json.load(file)\n",
    "                all_titles.extend(titles)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"파일 '{input_file}'을(를) 찾을 수 없습니다.\")\n",
    "        current_date += timedelta(days=1)\n",
    "    # 모든 제목을 하나의 JSON 파일로 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(all_titles, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"모든 뉴스 제목이 '{output_file}' 파일에 저장되었습니다.\")\n",
    "\n",
    "# 실행\n",
    "query = \"아시아나항공\"\n",
    "start_date = \"2024.07.02\"\n",
    "end_date = \"2024.07.02\"\n",
    "# 날짜별 JSON 파일 저장\n",
    "save_daily_news(query, start_date, end_date)\n",
    "# 모든 JSON 파일 병합\n",
    "# merge_json_files(start_date, end_date, \"all_asiana_news_titles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for date 2024.07.09...\n",
      "Fetching page 2 for date 2024.07.09...\n",
      "Fetching page 3 for date 2024.07.09...\n",
      "Fetching page 4 for date 2024.07.09...\n",
      "Fetching page 5 for date 2024.07.09...\n",
      "Fetching page 6 for date 2024.07.09...\n",
      "Fetching page 7 for date 2024.07.09...\n",
      "더 이상 뉴스가 없습니다.\n",
      "56개의 뉴스 제목이 'news_titles_2024.07.09.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "def get_news_titles(query, date):\n",
    "    base_url = \"https://search.naver.com/search.naver\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
    "    }\n",
    "    params = {\n",
    "        \"where\": \"news\",\n",
    "        \"query\": query,\n",
    "        \"sm\": \"tab_opt\",\n",
    "        \"sort\": \"0\",  # 관련도순 정렬\n",
    "        \"pd\": \"3\",  # 기간 설정\n",
    "        \"ds\": date,  # 시작 날짜\n",
    "        \"de\": date,  # 종료 날짜\n",
    "        \"start\": 1,  # 첫 페이지\n",
    "    }\n",
    "    titles = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Fetching page {page} for date {date}...\")\n",
    "            params[\"start\"] = (page - 1) * 10 + 1  # 페이지 계산\n",
    "            response = requests.get(base_url, headers=headers, params=params)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"HTTP 요청 실패: {response.status_code}\")\n",
    "                break\n",
    "            # HTML 파싱\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # 뉴스 제목 추출\n",
    "            news_items = soup.select(\".news_tit\")  # 제목 링크에 해당하는 클래스\n",
    "            if not news_items:\n",
    "                print(\"더 이상 뉴스가 없습니다.\")\n",
    "                break\n",
    "            for item in news_items:\n",
    "                title = item.get(\"title\")\n",
    "                if title:\n",
    "                    titles.append(title)\n",
    "            # 다음 페이지로 이동\n",
    "            page += 1\n",
    "            # 네이버 뉴스 검색은 최대 400개 기사만 제공\n",
    "            if page > 40:\n",
    "                print(\"최대 페이지 제한 도달.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"페이지 {page} 크롤링 중 에러 발생: {e}\")\n",
    "            break\n",
    "    return titles\n",
    "def save_daily_news(query, start_date, end_date):\n",
    "    current_date = datetime.strptime(start_date, \"%Y.%m.%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y.%m.%d\")\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y.%m.%d\")\n",
    "        titles = get_news_titles(query, date_str)\n",
    "        # 날짜별 JSON 파일 저장\n",
    "        output_file = f\"news_titles_{date_str}.json\"\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(titles, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"{len(titles)}개의 뉴스 제목이 '{output_file}' 파일에 저장되었습니다.\")\n",
    "        # 다음 날짜로 이동\n",
    "        current_date += timedelta(days=1)\n",
    "def merge_json_files(start_date, end_date, output_file):\n",
    "    current_date = datetime.strptime(start_date, \"%Y.%m.%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y.%m.%d\")\n",
    "    all_titles = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y.%m.%d\")\n",
    "        input_file = f\"news_titles_{date_str}.json\"\n",
    "        try:\n",
    "            with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                titles = json.load(file)\n",
    "                all_titles.extend(titles)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"파일 '{input_file}'을(를) 찾을 수 없습니다.\")\n",
    "        current_date += timedelta(days=1)\n",
    "    # 모든 제목을 하나의 JSON 파일로 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(all_titles, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"모든 뉴스 제목이 '{output_file}' 파일에 저장되었습니다.\")\n",
    "# 실행\n",
    "query = \"아시아나항공\"\n",
    "start_date = \"2024.07.09\"\n",
    "end_date = \"2024.07.09\"\n",
    "# 날짜별 JSON 파일 저장\n",
    "save_daily_news(query, start_date, end_date)\n",
    "# 모든 JSON 파일 병합\n",
    "# merge_json_files(start_date, end_date, \"all_asiana_news_titles.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_data_collection_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
